{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Modeling : Deep Learning Show Cases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMIYKN+Pf4Yj179lb19K3d4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H6hSvYjpmyp",
        "colab_type": "text"
      },
      "source": [
        "# Setting Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48qrusPy0KMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "path = './gdrive/My Drive/Colab Demo Data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgWFiLpx9skV",
        "colab_type": "code",
        "outputId": "df3f04ec-df38-4cc2-c627-2222d408ff78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check if GPU working\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLr4RqGMQafD",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzOQomF0Qnne",
        "colab_type": "code",
        "outputId": "fbcb723d-98be-4bcc-b999-095b0b887a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "dft = df_tweets_pub\n",
        "Y_label = 'sent_TextBlob'  #sent_SWN, sent_TextBlob\n",
        "\n",
        "import seaborn as sns\n",
        "sns.countplot(x= Y_label, data=dft)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fda9529b2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEHCAYAAABBW1qbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naiwFSVLzf5EzrK1clfN4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d8knVQz23g0",
        "colab_type": "text"
      },
      "source": [
        "## Pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8eKlAf9Lv52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from numpy import array\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKnj7Wyf3yM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=[]\n",
        "sentences = list(dft['tweets'])\n",
        "\n",
        "for sen in sentences:\n",
        "    X.append(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_PxSJ3UbCql",
        "colab_type": "code",
        "outputId": "2968517d-48bf-4a21-c898-a2e0af93aad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "no_Y = dft[Y_label].nunique()\n",
        "\n",
        "print(set(dft[Y_label]))\n",
        "print(no_Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, -1}\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRLJaCRvZXI2",
        "colab_type": "code",
        "outputId": "749af573-41db-4c30-8a09-d199c056cd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Label Encoding\n",
        "label_map = {0: 0, -1: -1, 1: 1}\n",
        "\n",
        "Y = dft[Y_label].map(label_map)\n",
        "set(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{-1, 0, 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l0DoXWS27OI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 資料切割：Train & Test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y , test_size=0.3, random_state=320) \n",
        "\n",
        "\"\"\"\n",
        "X_train=X_train.drop(columns=[Y_label])\n",
        "X_test =X_test.drop(columns=[Y_label])\n",
        "\n",
        "# 查看結果\n",
        "print(X_train.shape) \n",
        "print(X_test.shape)\n",
        "print(y_train.shape) \n",
        "print(y_test.shape)\n",
        "\"\"\"\n",
        "\n",
        "# 打亂資料集 (增加亂數性，讓模型訓練更好)\n",
        "X_train, y_train = shuffle(X_train, y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b735i12FbAnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keep Label encoding form for Confusion Matrix\n",
        "y_train_LBE = y_train\n",
        "y_test_LBE  = y_test\n",
        "\n",
        "# One-Hot Encoding for Train NN\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=no_Y)\n",
        "y_test  = to_categorical(y_test, num_classes=no_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDfqTTnDLCwB",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH3a7Xj1LGWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j63W-dzzLasx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUTh4k2jHy_U",
        "colab_type": "text"
      },
      "source": [
        "### Enbedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt3RRgR8HxPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open(path + 'glove6b100dtxt/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5aBxvhxIvWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnDMVuxuMEkO",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPOlHw2s2Q5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "from keras.layers.embeddings import Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Q0CUTeucsZ",
        "colab_type": "text"
      },
      "source": [
        "### Regularizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6sJ84lyuih7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 設定 regularizer \n",
        "''' Import l1,l2 (regularizer) '''\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "\n",
        "import keras.backend as K\n",
        "# l1_l2_list = [(0, 0), (0.0005, 0), (0, 0.0005), (0.0005, 0.0005), (0, 0.005)] #@param {type:\"raw\"}\n",
        "\n",
        "l1_alpha = 0.0005\n",
        "l2_alpha = 0.0005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsrIsSbLOMNY",
        "colab_type": "text"
      },
      "source": [
        "### Model 1: Simple NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuDf8fphOLxs",
        "colab_type": "code",
        "outputId": "6aeea814-3a7a-4c1b-9bd3-1616c1d22b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model_NN = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model_NN.add(embedding_layer)\n",
        "model_NN.add(Flatten())\n",
        "model_NN.add(Dense(no_Y, activation='sigmoid', kernel_regularizer=l1_l2(l1=l1_alpha, l2=l2_alpha)))\n",
        "\n",
        "print(model_NN.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 100, 100)          19200     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3)                 30003     \n",
            "=================================================================\n",
            "Total params: 49,203\n",
            "Trainable params: 30,003\n",
            "Non-trainable params: 19,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHgiXI1TjUNf",
        "colab_type": "text"
      },
      "source": [
        "### Model 2: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-uQXJWaIJzi",
        "colab_type": "code",
        "outputId": "445108e3-6205-4573-ed0d-c1829faa04bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "\n",
        "model_CNN = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model_CNN.add(embedding_layer)\n",
        "\n",
        "model_CNN.add(Conv1D(128, 5, activation='relu'))\n",
        "model_CNN.add(GlobalMaxPooling1D())\n",
        "model_CNN.add(Dense(no_Y, activation='sigmoid', kernel_regularizer=l1_l2(l1=l1_alpha, l2=l2_alpha)))\n",
        "\n",
        "print(model_CNN.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 100, 100)          19200     \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 96, 128)           64128     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 83,715\n",
            "Trainable params: 64,515\n",
            "Non-trainable params: 19,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJsrsIyuMy6e",
        "colab_type": "text"
      },
      "source": [
        "### Model 3: RNN (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVcF7ITEMyDS",
        "colab_type": "code",
        "outputId": "3be3ebcb-6b29-4703-b062-46cd5fd6bd6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model_LSTM = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model_LSTM.add(embedding_layer)\n",
        "model_LSTM.add(LSTM(128))\n",
        "\n",
        "model_LSTM.add(Dense(no_Y, activation='sigmoid', kernel_regularizer=l1_l2(l1=l1_alpha, l2=l2_alpha)))\n",
        "\n",
        "print(model_LSTM.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 100, 100)          19200     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 136,835\n",
            "Trainable params: 117,635\n",
            "Non-trainable params: 19,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gxFtwOr2lPq",
        "colab_type": "text"
      },
      "source": [
        "### Model 4: DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bnGc6Q72qcO",
        "colab_type": "code",
        "outputId": "4bc704f0-0744-4542-a624-2a13ce25a034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "model_DNN = Sequential()\n",
        "\n",
        "# 活化函數清單 acti_list = ['sigmoid', 'relu', 'tanh', 'softsign'] #@param {type:\"raw\"}\n",
        "\n",
        "# 加入第一層 hidden layer (128 neurons) 與指定 input 的維度\n",
        "model_DNN.add(Dense(128, input_dim=X_train.shape[1], kernel_regularizer=l1_l2(l1=l1_alpha, l2=l2_alpha)))\n",
        "model_DNN.add(Activation('relu'))\n",
        "\n",
        "# 加入第二層 hidden layer (256 neurons)\n",
        "model_DNN.add(Dense(256, kernel_regularizer=l1_l2(l1=l1_alpha, l2=l2_alpha)))\n",
        "model_DNN.add(Activation('relu'))\n",
        "\n",
        "# 加入第三層 hidden layer (512 neurons)\n",
        "model_DNN.add(Dense(512, kernel_regularizer=l1_l2(l1=l1_alpha, l2=l2_alpha)))\n",
        "model_DNN.add(Activation('relu'))\n",
        "\n",
        "# 加入 output layer (5 neurons)\n",
        "model_DNN.add(Dense(no_Y, kernel_regularizer=l1_l2(l1=l1_alpha, l2=l2_alpha)))\n",
        "model_DNN.add(Activation('softmax'))\n",
        "\n",
        "# 觀察 model summary\n",
        "print(model_DNN.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_26 (Dense)             (None, 128)               12928     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 3)                 1539      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 179,075\n",
            "Trainable params: 179,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C4UceNMwLEJ",
        "colab_type": "text"
      },
      "source": [
        "## Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82lp8Kq3w53L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_funcs = ['mean_squared_error', 'mean_absolute_error', 'categorical_crossentropy', 'binary_crossentropy']  #@param {type:\"raw\"}\n",
        "\n",
        "loss_fun = 'categorical_crossentropy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmKtNYZSxMTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "optim = 'Adam'\n",
        "\n",
        "# lr(學習率) - 是一種超參數，它控制了我們在大多程度上調整了我們的網路權重。並對損失梯度進行調整。\n",
        "# momentum - 動量來源於牛頓定律，基本思想是為了找到最優加入“慣性”的影響，當誤差曲面中存在平坦區域，SGD可以更快的學習。\n",
        "# decay - 該方法是為了提高SGD尋優能力，具體就是每次迭代的時候減少學習率的大小。\n",
        "\n",
        "# https://keras.io/optimizers/\n",
        "\n",
        "# Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "# SGD(lr=10e-3, momentum=0.0, decay=0.0, nesterov=False)\n",
        "# Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "# RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "# Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
        "# Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
        "# Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w49Pvz0wW7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CandidateModel = model_DNN  # model_NN, model_CNN, model_LSTM, model_DNN\n",
        "\n",
        "CandidateModel.compile(loss=loss_fun,\n",
        "                       optimizer=optim,\n",
        "                       metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g_WJBQCKAeD",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL2EdXyqOukK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 設定 batch_size，訓練迭代輪次(回合數) epochs\n",
        "batch_size = 64 # 1個mini-batch有n筆資料\n",
        "epochs = 20\n",
        "\n",
        "# 訓練開始\n",
        "import time\n",
        "start = time.process_time()\n",
        "\n",
        "fitting_history = CandidateModel.fit(X_train, y_train, \n",
        "                                     batch_size=batch_size, \n",
        "                                     epochs=epochs, \n",
        "                                     verbose=1,\n",
        "                                     shuffle=True, \n",
        "                                     validation_split=0.3)\n",
        "\n",
        "# 訓練結束\n",
        "end = time.process_time()\n",
        "print(\"訓練時間：%.3fs 秒\" % (end - start))\n",
        "\n",
        "# 測試結果\n",
        "predict =CandidateModel.predict_classes(X_test)  # 預測標籤結果\n",
        "predict_cross =CandidateModel.predict(X_test)    # 每個標籤的預測機率"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eEdQZvU3toV",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcYJH6iC14RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = CandidateModel.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"\\nLoss: %.3f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
        "\n",
        "#######################\n",
        "\n",
        "loss     = fitting_history.history.get('loss')      # 取出訓練完成後loss的訓練數據\n",
        "acc      = fitting_history.history.get('acc')       # 取出訓練完成後accuracy的訓練數據\n",
        "val_loss = fitting_history.history.get('val_loss')  # 取出訓練完成後val_loss的訓練數據\n",
        "val_acc  = fitting_history.history.get('val_acc')   # 取出訓練完成後val_accuracy的訓練數據\n",
        "\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 10, 7\n",
        "\n",
        "''' Visualize the loss and accuracy of both models'''\n",
        "plt.figure(0)\n",
        "plt.subplot(121)\n",
        "\n",
        "plt.plot(range(len(loss)), loss, label='Training loss')\n",
        "plt.plot(range(len(val_loss)), val_loss, label='validation loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(122)\n",
        "\n",
        "plt.plot(range(len(acc)), acc, label='Training accuracy')\n",
        "plt.plot(range(len(val_acc)), val_acc, label='validation accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VzZPMqeVLHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 製作 Confusion Matrix圖表來評估模型\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_true=y_test_LBE, y_pred=predict, labels=[-1,0,1])\n",
        "\n",
        "cm =pd.DataFrame(confusion_matrix , index = ['-1', '0', '1'], columns = [ '-1', '0', '1'])\n",
        "sns.heatmap(cm, annot = True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "Accuracy = confusion_matrix.trace() / confusion_matrix.sum()\n",
        "MissClass = 1 - Accuracy\n",
        "\n",
        "print(\"Accuracy : %0.5f\" % Accuracy)\n",
        "print(\"MissClass: %0.5f\" % MissClass)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B4m-m6badYo",
        "colab_type": "text"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvKSl9zQajaI",
        "colab_type": "code",
        "outputId": "42ab4e6e-66d1-4f9f-cf19-edbefd6777ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 儲存模型\n",
        "\n",
        "ymd = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
        "AC = Accuracy.astype('str')\n",
        "\n",
        "dump(m1, path + 'm1-'+ymd+'XGB'+ AC +'.joblib')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7fb9239b8358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZrwknU6SA8_",
        "colab_type": "text"
      },
      "source": [
        "# Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfYohh2uTaNB",
        "colab_type": "code",
        "outputId": "5ad11e5f-30b3-4f7b-db1a-31d2381716ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "instance = 'I believe that Python 3.1 will print them nicer by default, without any code changing. But that is useless if you use any extensions that havent been updated to work with Python 3.1'\n",
        "\n",
        "instance = tokenizer.texts_to_sequences(instance)\n",
        "\n",
        "flat_list = []\n",
        "for sublist in instance:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "\n",
        "flat_list = [flat_list]\n",
        "\n",
        "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
        "\n",
        "predict_p = CandidateModel.predict(instance)\n",
        "predict_c = CandidateModel.predict_classes(instance)  # 預測標籤結果\n",
        "\n",
        "print(predict_p)\n",
        "print(predict_c)\n",
        "\n",
        "if predict_c[0]==1:\n",
        "    print(' Positive')\n",
        "elif predict_c[0]==-1:\n",
        "    print(' Negative')\n",
        "else:\n",
        "    print(' Nature')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.48823264 0.5123518  0.4875249 ]]\n",
            "[1]\n",
            " Positive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}